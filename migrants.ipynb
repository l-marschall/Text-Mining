{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, VectorizerMixin\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_colwidth', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: you can add other languages that Spacy supports, or download\n",
    "# larger models for english that Spacy offers. \n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "def stop_word_removal(li):    \n",
    "    return [l for l in li if l not in ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def translation(s):\n",
    "    blob = TextBlob(s)\n",
    "    return format(blob.translate(to = 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from utils import clean_html\n",
    "from sklearn.feature_extraction.text import strip_accents_unicode\n",
    "import snowballstemmer\n",
    "\n",
    "def clean_twitter(s):\n",
    "    \"\"\" Cleans Twitter specific issues \n",
    "    \n",
    "    Can you think of what else you might need to add here?\n",
    "    \"\"\"\n",
    "    s = re.sub(r'@\\w+', '', s) #remove @ mentions from tweets    \n",
    "    return s\n",
    "\n",
    "def clean_tweet(s):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", s).split())\n",
    "# (@[A-Za-z0-9]+)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor(s):\n",
    "    \"\"\" For all basic string cleanup. \n",
    "    \n",
    "    Think of what you can add to this to improve things. What is\n",
    "    specific to your goal, how can you transform the text. Add tokens,\n",
    "    remove things, unify things. \n",
    "    \"\"\"\n",
    "    s = clean_html(s)\n",
    "    s = strip_accents_unicode(s.lower())\n",
    "    #s = clean_twitter(s)\n",
    "    s = clean_tweet(s)\n",
    "    \n",
    "    #stemmer = snowballstemmer.stemmer('english')\n",
    "    #s = stemmer.stemWords(s.split())\n",
    "    #s = ' '.join(s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die migrant'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cool_tokenizer(sent):\n",
    "    \"\"\" Idea from Travis in class: adds a token to the end with nsubj and root verb!\"\"\"\n",
    "    doc = nlp(sent)\n",
    "    tokens = sorted(doc, key = lambda t: t.dep_)\n",
    "    return ' '.join([t.lemma_ for t in tokens if t.dep_ in ['nsubj', 'ROOT']])\n",
    "\n",
    "cool_tokenizer('a migrant died in crossing the river')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['migrant:nsubj', 'die:ROOT', 'cross:pcomp', 'river:dobj']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from langdetect import detect\n",
    "\n",
    "def dep_tokenizer(sent):\n",
    "    \"\"\" A simple version of tokenzing with the dependencies.\n",
    "    \n",
    "    Note: this is monolingual! Also, it still doesn't take into \n",
    "    account correlations!\n",
    "    \"\"\"\n",
    "    doc = nlp(sent)\n",
    "    tokens = [t for t in doc if not t.is_stop and t.dep_ not in ['punct', '']]\n",
    "    tokens = [':'.join([t.lemma_,t.dep_]) for t in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "dep_tokenizer('a migrant died in crossing the river')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ONE'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokenizer(sent):\n",
    "    \"\"\" Idea that tweets with numerical words or numbers reflect death-tools or something similar!\"\"\"\n",
    "    doc = nlp(sent)\n",
    "    tokens = [t.pos_ for t in doc if t.pos_ in ['NUM']]\n",
    "    if tokens != []:\n",
    "        return('ONE')\n",
    "    else: return('ZERO')\n",
    "\n",
    "num_tokenizer('100 migrants died in crossing the river')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_tokenizer(sent):\n",
    "        '''\n",
    "        Utility function to classify sentiment of passed tweet\n",
    "        using textblob's sentiment method\n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text\n",
    "        analysis = TextBlob(clean_tweet(sent))\n",
    "        # set sentiment\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 'POSITIVE'\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 'NEUTRAL'\n",
    "        else:\n",
    "            return 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def analyzer(s, ngram_range = (1, 4)):\n",
    "    \n",
    "    \"\"\" Does everything to turn raw documents into tokens. \"\"\"\n",
    "    \n",
    "    sentiment_token = [sentiment_tokenizer(s)]\n",
    "    s = preprocessor(s)\n",
    "    pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    unigrams = pattern.findall(s)\n",
    "    #unigrams = stop_word_removal(unigrams)\n",
    "    cool_token = [cool_tokenizer(s)]\n",
    "    num_token = [num_tokenizer(s)]\n",
    "    tokens = ngrammer(unigrams, ngram_range) + cool_token + num_token + sentiment_token  \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = pd.read_csv('kaggle/train.csv').tweet\n",
    "#for i in range(0, len(X)):\n",
    "#    if clean_html(X[i]) != '':\n",
    "#        if detect(X[i]) != 'en':\n",
    "#            try:\n",
    "#                X[i] = translation(X[i])\n",
    "#            except:\n",
    "#                pass\n",
    "\n",
    "#X.to_csv('kaggle/X_eng.csv', header = 'tweet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.read_csv('kaggle/X_eng.csv', lineterminator = '\\n').tweet\n",
    "y = pd.read_csv('kaggle/train.csv').label\n",
    "\n",
    "#X = pd.read_csv('kaggle/kaggle_train.csv').tweet\n",
    "#y = pd.read_csv('kaggle/kaggle_train.csv').label\n",
    "\n",
    "cutoff = 1750\n",
    "X_train, X_test, y_train, y_test = X[0:cutoff], X[cutoff:], y[0:cutoff], y[cutoff:]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1750,), (450,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "def create_vectors(X_train, X_test, analyzer = analyzer):\n",
    "    \"\"\" Just a small helper function that applies the SKLearn Vectorizer with our analyzer \"\"\"\n",
    "    idx = X_train.shape[0]\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    vectorizer = TfidfVectorizer(analyzer=analyzer).fit(X)\n",
    "    #vectorizer = CountVectorizer(analyzer=analyzer).fit(X)\n",
    "    vector = vectorizer.transform(X)\n",
    "\n",
    "    return vector[0:idx], vector[idx:], vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hot_coding_sentiment(X, matrix, sentiment):\n",
    "    rows = matrix.shape[0]\n",
    "    vector = np.zeros(rows, int)\n",
    "    vector.shape = (rows, 1)\n",
    "    for tweet in range(0, len(X)):\n",
    "        if sentiment_tokenizer(X[tweet]) == sentiment:\n",
    "            vector[tweet] = 1\n",
    "    matrix = np.hstack([matrix, vector])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hot_coding_numerical(X, matrix, numerical):\n",
    "    rows = matrix.shape[0]\n",
    "    vector = np.zeros(rows, int)\n",
    "    vector.shape = (rows, 1)\n",
    "    for tweet in range(0, len(X)):\n",
    "        if num_tokenizer(X[tweet]) == numerical:\n",
    "            vector[tweet] = 1\n",
    "    matrix = np.hstack([matrix, vector])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hot_coding(X, V):\n",
    "    sentiment = ['positive', 'negative']\n",
    "    numerical = ['one']\n",
    "    for sen in sentiment:\n",
    "        V = hot_coding_sentiment(X, V, sen)\n",
    "    for num in numerical:\n",
    "        V = hot_coding_numerical(X, V, num)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "V_train, V_test, vectorizer = create_vectors(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#V_train = V_train.toarray()\n",
    "#V_test = V_test.toarray()\n",
    "\n",
    "#V_train = hot_coding(X_train, V_train)\n",
    "#X_test_reset = X_test.reset_index().tweet\n",
    "#V_test = hot_coding(X_test_reset, V_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(V_train, y_train)\n",
    "preds = model.predict_proba(V_test)[:,1]\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB(class_prior=[0.5, 0.5])\n",
    "model.fit(V_train, y_train)\n",
    "preds = model.predict_proba(V_test)[:,1]\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "one_to_left = st.beta(10, 1)\n",
    "from_zero_positive = st.expon(0, 1)\n",
    "params = {\n",
    "    \"C\": from_zero_positive,\n",
    "    \"intercept_scaling\": from_zero_positive,\n",
    "}\n",
    "\n",
    "model = LinearSVC(tol= 10e-6, max_iter= 1000, \n",
    "                  penalty= 'l2', loss= 'squared_hinge', \n",
    "                  dual= True, fit_intercept= True, \n",
    "                  random_state= None, class_weight= None, \n",
    "                  verbose= 0)\n",
    "model = RandomizedSearchCV(model, params, n_jobs=1, n_iter=20)\n",
    "\n",
    "model.fit(V_train, y_train)\n",
    "preds = model.decision_function(V_test)\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8942948750572549"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(tol = 10e-6, max_iter = 100,\n",
    "                  penalty = 'l2', loss = 'squared_hinge', \n",
    "                  dual = True, C = 0.001, \n",
    "                  fit_intercept = True, intercept_scaling = 10,\n",
    "                  verbose = 0, random_state = None, \n",
    "                  class_weight = None)\n",
    "\n",
    "model.fit(V_train, y_train)\n",
    "preds = model.decision_function(V_test)\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "one_to_left = st.beta(10, 1)\n",
    "from_zero_positive = st.expon(0, 1)\n",
    "params = {\n",
    "    \"n_estimators\": st.randint(50, 500),\n",
    "    \"max_depth\": st.randint(1, 3),\n",
    "    \"learning_rate\": st.uniform(0.01, 0.8),\n",
    "    \"subsample\": one_to_left,\n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc = RandomizedSearchCV(gbc, params, n_jobs=1, n_iter=5)\n",
    "\n",
    "#gbc = GradientBoostingClassifier(n_estimators = 300, max_depth = 1, learning_rate = 0.1)\n",
    "gbc.fit(V_train, y_train)\n",
    "pred = gbc.predict(V_test)\n",
    "gbc.predict_proba(V_test)\n",
    "gbc.score(V_test, y_test)\n",
    "\n",
    "#preds = gbc.decision_function(V_test)\n",
    "#roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "one_to_left = st.beta(10, 1)\n",
    "from_zero_positive = st.expon(0, 1)\n",
    "params = {\n",
    "    \"n_estimators\": st.randint(50, 500),\n",
    "    \"max_depth\": st.randint(1, 3),\n",
    "    \"learning_rate\": st.uniform(0.01, 0.8),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "xgbc = XGBClassifier(nthreads=-1)\n",
    "xgbc = RandomizedSearchCV(xgbc, params, n_jobs=1, n_iter=10)\n",
    "xgbc.fit(V_train, y_train)\n",
    "pred = xgbc.predict(V_test)\n",
    "xgbc.predict_proba(V_test)\n",
    "xgbc.score(V_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at your false predictions!\n",
    "false_pos, false_neg = get_errors(X_test, y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission!\n",
    "\n",
    "Here you can make the submission required for Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_sub = pd.read_csv('kaggle/test.csv').tweet\n",
    "#for i in range(0, len(X_sub)):\n",
    "#    if clean_html(X_sub[i]) != '':\n",
    "#        if detect(X_sub[i]) != 'en':\n",
    "#            try:\n",
    "#                X_sub[i] = translation(X_sub[i])\n",
    "#            except:\n",
    "#                pass\n",
    "\n",
    "#X_sub.to_csv('kaggle/test_X_eng.csv', header = 'tweet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sub = pd.read_csv('kaggle/test_X_eng.csv', lineterminator = '\\n').tweet\n",
    "id_sub = pd.read_csv('kaggle/test.csv').id\n",
    "\n",
    "V_train, V_test, _ = create_vectors(X, X_sub)\n",
    "#V_train = V_train.toarray()\n",
    "#V_test = V_test.toarray()\n",
    "\n",
    "#V_train = hot_coding(X_train, V_train)\n",
    "#X_test_reset = X_test.reset_index().tweet\n",
    "#V_test = hot_coding(X_test_reset, V_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(V_train, y)\n",
    "preds = model.decision_function(V_test)\n",
    "#gbc.fit(V_train, y)\n",
    "#preds = gbc.decision_function(V_test)\n",
    "\n",
    "#xgbc.fit(V_train, y)\n",
    "#preds = xgbc.predict_proba(V_test)\n",
    "#preds = [x[1] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_submission_csv(preds, id_sub, 'kaggle/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
